{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56da08c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We Have a Techniques to convert (Human Text) Text into a meaningful vectors.\\nvectors nothing but the numbers, vectors represent the meaningful information with respect to pirticular data'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"We Have a Techniques to convert (Human Text) Text into a meaningful vectors.\n",
    "vectors nothing but the numbers, vectors represent the meaningful information with respect to pirticular data\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00a1a604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Input data in the form of text or sentence . we baciscally we use NLP.  \\nwe will be able to process this pirticular data and able to make understand the mode to solve the use cases like spam classification'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Input data in the form of text or sentence . we baciscally we use NLP.  \n",
    "we will be able to process this pirticular data and able to make understand the mode to solve the use cases like spam classification\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "b6f5d3fa",
   "metadata": {},
   "source": [
    "Step :1     Text [preprocessing\n",
    "\n",
    "             Tokenization, lemmatization, stemming, stop words\n",
    "        \n",
    "step :2     BagOfWords  TFIDF, Unigram,  Bigram\n",
    "\n",
    "step :3     Word2Vec,  AvgWord2Vec\n",
    "\n",
    "step :4     RNN, LSTM RNN, GRUNN\n",
    "\n",
    "step :5     TextPreprocessing--->Word embedding\n",
    "\n",
    "\n",
    "step:6      Transformers \n",
    "\n",
    "steP :7      BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cdedb3",
   "metadata": {},
   "source": [
    "# Tokensization\n",
    "\n",
    "it is aprocess of converting either a paragraph or a sentence into tokens.\n",
    "basically if an converting a paragraph into sentence i can also convert into a words "
   ]
  },
  {
   "cell_type": "raw",
   "id": "c32ab962",
   "metadata": {},
   "source": [
    "1 CORPUS:---->Paragraph\n",
    "    \n",
    "2 DOCUMENTS:---->Sentences\n",
    "    \n",
    "3  VOCABULARY:-----> Unique WOrds\n",
    "    \n",
    "4 TOPICS:---->Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecfe32de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\naga\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\naga\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: click in c:\\users\\naga\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\naga\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\naga\\anaconda3\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\naga\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d32d6a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\naga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2529c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"Hello welcome to the United KIngdom. hope you will enjoy the weather! please don't hesitate to call me if you nedd.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e16ac052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "529afc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60c0ce7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc36d9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello welcome to the United KIngdom.',\n",
       " 'hope you will enjoy the weather!',\n",
       " \"please don't hesitate to call me if you nedd.\"]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b13f8291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello welcome to the United KIngdom.\n",
      "hope you will enjoy the weather!\n",
      "please don't hesitate to call me if you nedd.\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc7a6bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'United',\n",
       " 'KIngdom',\n",
       " '.',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'will',\n",
       " 'enjoy',\n",
       " 'the',\n",
       " 'weather',\n",
       " '!',\n",
       " 'please',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'hesitate',\n",
       " 'to',\n",
       " 'call',\n",
       " 'me',\n",
       " 'if',\n",
       " 'you',\n",
       " 'nedd',\n",
       " '.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95174155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'welcome', 'to', 'the', 'United', 'KIngdom', '.']\n",
      "['hope', 'you', 'will', 'enjoy', 'the', 'weather', '!']\n",
      "['please', 'do', \"n't\", 'hesitate', 'to', 'call', 'me', 'if', 'you', 'nedd', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c32b7a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'United',\n",
       " 'KIngdom',\n",
       " '.',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'will',\n",
       " 'enjoy',\n",
       " 'the',\n",
       " 'weather',\n",
       " '!',\n",
       " 'please',\n",
       " 'don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'hesitate',\n",
       " 'to',\n",
       " 'call',\n",
       " 'me',\n",
       " 'if',\n",
       " 'you',\n",
       " 'nedd',\n",
       " '.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "713e3dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'United',\n",
       " 'KIngdom.',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'will',\n",
       " 'enjoy',\n",
       " 'the',\n",
       " 'weather',\n",
       " '!',\n",
       " 'please',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'hesitate',\n",
       " 'to',\n",
       " 'call',\n",
       " 'me',\n",
       " 'if',\n",
       " 'you',\n",
       " 'nedd',\n",
       " '.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fade5c",
   "metadata": {},
   "source": [
    "#  STEMMING\n",
    "\n",
    "it is the process of reducing the word to its word stem that affixes to prefixes and suffixes or the roots of words known as lemma. Stemming is important in Natural Language Understanding (NLU) and Natural Processsing Language(NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e76b01ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "##classification problesm\n",
    "#comments of the product +ve or -ve reviews \n",
    "#reviews--> eating, eat, eaten --->eat is a word stem\n",
    "#going, goes ,gone----> go is the word stem\n",
    "\n",
    "words=[\"eating\", \"eats\", \"eaten\", \"writing\", \"writes\", \"programming\", \"programs\", \"history\", \"finally\", \"finalized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "609217c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "eats---->eat\n",
      "eaten---->eaten\n",
      "writing---->write\n",
      "writes---->write\n",
      "programming---->program\n",
      "programs---->program\n",
      "history---->histori\n",
      "finally---->final\n",
      "finalized---->final\n"
     ]
    }
   ],
   "source": [
    "#stemming techniques are many one of them is \n",
    "#1 porterStemmmer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming=PorterStemmer()\n",
    "for word in words:\n",
    "    print(word+\"---->\"+ stemming.stem(word))## here you can history as histori(major disadantage in stemming is changd the meaning of word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd5d1164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem(\"congratulation\")# this is all get fixed with the help of lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b375f7",
   "metadata": {},
   "source": [
    "# RegexStemmer class\n",
    "\n",
    "NLTK has a RegexStemmer class with the help of which we can easily implement Regular Expression Stemmer algorithms. It is basically takes a single regular expression and removes any prefix or suffix and matches the expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8fefc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "faf8458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stemmer=RegexpStemmer('ing$|s$|able$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "782794bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem(\"eating\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9ebc8ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ingeat'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem(\"ingeating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ac987495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'suces'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem(\"sucess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dfff59c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ablemanage'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem(\"ablemanageable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf8de99",
   "metadata": {},
   "source": [
    "# snowball stemmer\n",
    "It performs better than Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a1d819d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "840f5a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowballstemmer=SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d3c54b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating--->eat\n",
      "eats--->eat\n",
      "eaten--->eaten\n",
      "writing--->write\n",
      "writes--->write\n",
      "programming--->program\n",
      "programs--->program\n",
      "history--->histori\n",
      "finally--->final\n",
      "finalized--->final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"--->\"+ snowballstemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "639ee8e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fairli'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem(\"fairly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "89dd4683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fair'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowballstemmer.stem(\"fairly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8310f0",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "it is a techniques is like stemming. the output we will get after lemmatization is called lemma. which is root word rather than root stem, the output of stemming. After lemmatization we will be getting a valid word that means something "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b20c26b",
   "metadata": {},
   "source": [
    "NLTK provides WordNetLemmatize class which is a thin wrapper around the wordnet corpus. this class uses morphy() function to the wordnet corpus reader class to fina a lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d5beea49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\naga\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "aba2a7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\naga\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3c7753f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import  WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "119ac6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"going\", pos='v')# here we can define the word is noun, verb, adjective, adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "df5bd205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating--->eating\n",
      "eats--->eats\n",
      "eaten--->eaten\n",
      "writing--->writing\n",
      "writes--->writes\n",
      "programming--->programming\n",
      "programs--->program\n",
      "history--->history\n",
      "finally--->finally\n",
      "finalized--->finalized\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"--->\"+ lemmatizer.lemmatize(word, pos=\"n\"))\n",
    "#lemmatization wil take more time than stemming  ex : Q&A, chatbots, TextSummerization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d5cd65",
   "metadata": {},
   "source": [
    "#  Stop words \n",
    " nothing but which removes the unused or useless words from the pragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "04463749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\naga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a26a36a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Paragraph=\"\"\"Environmental health was defined in a 1989 document by the World Health Organization (WHO) as: Those aspects of human health and disease that are determined by factors in the environment. It is also referred to as the theory and practice of accessing and controlling factors in the environment that can potentially affect health.\n",
    "\n",
    "A 1990 WHO document states that environmental health, as used by the WHO Regional Office for Europe, \"includes both the direct pathological effects of chemicals, radiation and some biological agents, and the effects (often indirect) on health and well being of the broad physical, psychological, social and cultural environment, which includes housing, urban development, land use and transport.\"\n",
    "\n",
    "As of 2016, the WHO website on environmental health states that \"Environmental health addresses all the physical, chemical, and biological factors external to a person, and all the related factors impacting behaviours. It encompasses the assessment and control of those environmental factors that can potentially affect health. It is targeted towards preventing disease and creating health-supportive environments. This definition excludes behaviour not related to environment, as well as behaviour related to the social and cultural environment, as well as genetics.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "17e21b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "42edcc3b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "272bf4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e9839742",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "cbef0049",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=nltk.sent_tokenize(Paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a37ad058",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply stop words than apply stemming \n",
    "for i in range(len(sentence)):\n",
    "    words=nltk.word_tokenize(sentence[i])\n",
    "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words(\"english\"))]\n",
    "    sentence[i]=' '.join(words)\n",
    "    #exacrtly converting words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "cc9dc83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['environmenthealthdefin1989documentworldhealthorgan ( ) : thoseaspecthumanhealthdiseasdeterminfactorenviron .',\n",
       " 'italsorefertheoripracticaccesscontrolfactorenvironpotentiaffecthealth .',\n",
       " \"a1990whodocumentstateenvironmenthealth , usewhoregionofficeurop , `` includdirectpathologeffectchem , radiatbiologag , effect ( oftenindirect ) healthwellbroadphys , psycholog , socialculturenviron , includh , urbandevelop , landusetransport . ''\",\n",
       " 'as2016 , whowebsitenvironmenthealthst `` environmenthealthaddressphys , chemic , biologfactorexternperson , relatfactorimpactbehaviour .',\n",
       " 'itencompassassesscontrolenvironmentfactorpotentiaffecthealth .',\n",
       " 'ittargettowardpreventdiseascreathealth-supportenviron .',\n",
       " 'thidefinitexcludbehaviourrelatenviron , wellbehaviourrelatsocialculturenviron , wellgenet .']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3bbfbb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowballstemmer=SnowballStemmer(\"english\")\n",
    "#apply stop words than apply stemming \n",
    "for i in range(len(sentence)):\n",
    "    words=nltk.word_tokenize(sentence[i])\n",
    "    words=[snowballstemmer.stem(word) for word in words if word not in set(stopwords.words(\"english\"))]\n",
    "    sentence[i]=' '.join(words)\n",
    "    #exacrtly converting words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "6cc1d408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['environment health defin 1989 document world health organ ( who ) : those aspect human health diseas determin factor environ .',\n",
       " 'it also refer theori practic access control factor environ potenti affect health .',\n",
       " \"a 1990 who document state environment health , use who region offic europ , `` includ direct patholog effect chemic , radiat biolog agent , effect ( often indirect ) health well broad physic , psycholog , social cultur environ , includ hous , urban develop , land use transport . ''\",\n",
       " 'as 2016 , who websit environment health state `` environment health address physic , chemic , biolog factor extern person , relat factor impact behaviour .',\n",
       " 'it encompass assess control environment factor potenti affect health .',\n",
       " 'it target toward prevent diseas creat health-support environ .',\n",
       " 'this definit exclud behaviour relat environ , well behaviour relat social cultur environ , well genet .']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a67bb69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import  WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "for i in range(len(sentence)):\n",
    "    words=nltk.word_tokenize(sentence[i])\n",
    "    words=[lemmatizer.lemmatize(word.lower(), pos=\"v\") for word in words if word not in set(stopwords.words(\"english\"))]\n",
    "    sentence[i]=' '.join(words)\n",
    "    #exacrtly converting words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6be3db8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['environmental health define 1989 document world health organization ( who ) : those aspects human health disease determine factor environment .',\n",
       " 'it also refer theory practice access control factor environment potentially affect health .',\n",
       " 'a 1990 who document state environmental health , use who regional office europe , `` include direct pathological effect chemicals , radiation biological agents , effect ( often indirect ) health well broad physical , psychological , social cultural environment , include house , urban development , land use transport . ``',\n",
       " 'as 2016 , who website environmental health state `` environmental health address physical , chemical , biological factor external person , relate factor impact behaviours .',\n",
       " 'it encompass assessment control environmental factor potentially affect health .',\n",
       " 'it target towards prevent disease create health-supportive environments .',\n",
       " 'this definition exclude behaviour relate environment , well behaviour relate social cultural environment , well genetics .']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fa1784",
   "metadata": {},
   "source": [
    "# Parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "698dcc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "Paragraph=\"\"\"Environmental health was defined in a 1989 document by the World Health Organization (WHO) as: Those aspects of human health and disease that are determined by factors in the environment. It is also referred to as the theory and practice of accessing and controlling factors in the environment that can potentially affect health.\n",
    "\n",
    "A 1990 WHO document states that environmental health, as used by the WHO Regional Office for Europe, \"includes both the direct pathological effects of chemicals, radiation and some biological agents, and the effects (often indirect) on health and well being of the broad physical, psychological, social and cultural environment, which includes housing, urban development, land use and transport.\"\n",
    "\n",
    "As of 2016, the WHO website on environmental health states that \"Environmental health addresses all the physical, chemical, and biological factors external to a person, and all the related factors impacting behaviours. It encompasses the assessment and control of those environmental factors that can potentially affect health. It is targeted towards preventing disease and creating health-supportive environments. This definition excludes behaviour not related to environment, as well as behaviour related to the social and cultural environment, as well as genetics.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "5f4e720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "61d73002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\naga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "69456cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=nltk.sent_tokenize(Paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "c594732d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'isdigit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[207], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m words\u001b[38;5;241m=\u001b[39mnltk\u001b[38;5;241m.\u001b[39mword_tokenize(sentence[i])\n\u001b[0;32m      3\u001b[0m words\u001b[38;5;241m=\u001b[39m[words \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m)) ]\n\u001b[1;32m----> 4\u001b[0m Pos_tag[i]\u001b[38;5;241m=\u001b[39m\u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(pos_tag[i])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py:166\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03mUse NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03mtag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m:rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    165\u001b[0m tagger \u001b[38;5;241m=\u001b[39m _get_tagger(lang)\n\u001b[1;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py:123\u001b[0m, in \u001b[0;36m_pos_tag\u001b[1;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens: expected a list of strings, got a string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 123\u001b[0m     tagged_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtagger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tagset:  \u001b[38;5;66;03m# Maps to the specified tagset.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m lang \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py:180\u001b[0m, in \u001b[0;36mPerceptronTagger.tag\u001b[1;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[0;32m    177\u001b[0m prev, prev2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTART\n\u001b[0;32m    178\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 180\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTART \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEND\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokens):\n\u001b[0;32m    182\u001b[0m     tag, conf \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    183\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagdict\u001b[38;5;241m.\u001b[39mget(word), \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m use_tagdict \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    184\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py:180\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    177\u001b[0m prev, prev2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTART\n\u001b[0;32m    178\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 180\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTART \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEND\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokens):\n\u001b[0;32m    182\u001b[0m     tag, conf \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    183\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagdict\u001b[38;5;241m.\u001b[39mget(word), \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m use_tagdict \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    184\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py:277\u001b[0m, in \u001b[0;36mPerceptronTagger.normalize\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m word \u001b[38;5;129;01mand\u001b[39;00m word[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!HYPHEN\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mword\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misdigit\u001b[49m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!YEAR\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mand\u001b[39;00m word[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39misdigit():\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'isdigit'"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentence)):\n",
    "    words=nltk.word_tokenize(sentence[i])\n",
    "    words=[words for word in words if word not in set(stopwords.words(\"english\")) ]\n",
    "    Pos_tag=nltk.pos_tag(words)\n",
    "    print(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28046f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cee329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
